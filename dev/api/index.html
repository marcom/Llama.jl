<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>API Reference · LlamaCpp.jl</title><meta name="title" content="API Reference · LlamaCpp.jl"/><meta property="og:title" content="API Reference · LlamaCpp.jl"/><meta property="twitter:title" content="API Reference · LlamaCpp.jl"/><meta name="description" content="Documentation for LlamaCpp.jl."/><meta property="og:description" content="Documentation for LlamaCpp.jl."/><meta property="twitter:description" content="Documentation for LlamaCpp.jl."/><meta property="og:url" content="https://marcom.github.io/LlamaCpp.jl/api/"/><meta property="twitter:url" content="https://marcom.github.io/LlamaCpp.jl/api/"/><link rel="canonical" href="https://marcom.github.io/LlamaCpp.jl/api/"/><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../search_index.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../">LlamaCpp.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li class="is-active"><a class="tocitem" href>API Reference</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>API Reference</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>API Reference</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/marcom/LlamaCpp.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/marcom/LlamaCpp.jl/blob/main/docs/src/api.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="API-Reference"><a class="docs-heading-anchor" href="#API-Reference">API Reference</a><a id="API-Reference-1"></a><a class="docs-heading-anchor-permalink" href="#API-Reference" title="Permalink"></a></h1><p>API Reference for <a href="https://github.com/marcom/LlamaCpp.jl">LlamaCpp</a>.</p><ul><li><a href="#LlamaCpp.download_model-Tuple{AbstractString}"><code>LlamaCpp.download_model</code></a></li><li><a href="#LlamaCpp.embeddings-Tuple{LlamaCpp.LlamaContext}"><code>LlamaCpp.embeddings</code></a></li><li><a href="#LlamaCpp.llama_eval-Tuple{LlamaCpp.LlamaContext, Vector{Int32}}"><code>LlamaCpp.llama_eval</code></a></li><li><a href="#LlamaCpp.logits-Tuple{LlamaCpp.LlamaContext}"><code>LlamaCpp.logits</code></a></li><li><a href="#LlamaCpp.run_chat-Tuple{}"><code>LlamaCpp.run_chat</code></a></li><li><a href="#LlamaCpp.run_llama-Tuple{}"><code>LlamaCpp.run_llama</code></a></li><li><a href="#LlamaCpp.run_server-Tuple{}"><code>LlamaCpp.run_server</code></a></li><li><a href="#LlamaCpp.token_to_str-Tuple{LlamaCpp.LlamaContext, Integer}"><code>LlamaCpp.token_to_str</code></a></li><li><a href="#LlamaCpp.tokenize-Tuple{LlamaCpp.LlamaContext, AbstractString}"><code>LlamaCpp.tokenize</code></a></li></ul><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="LlamaCpp.download_model-Tuple{AbstractString}" href="#LlamaCpp.download_model-Tuple{AbstractString}"><code>LlamaCpp.download_model</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">download_model(url::AbstractString; dir::AbstractString=&quot;models&quot;)</code></pre><p>Downloads a model specified by <code>url</code> from the HuggingFace Hub into <code>dir</code> directory and returns the <code>model_path</code> to the downloaded file. If the <code>dir</code> directory does not exist, it will be created.</p><p>Note: Currently allows only models in the GGUF format (expects the URL to end with <code>.gguf</code>).</p><p>See <a href="https://huggingface.co/models">HuggingFace Model Hub</a> for a list of available models.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs"># Download the Rocket model (~1GB)
url = &quot;https://huggingface.co/ikawrakow/various-2bit-sota-gguf/resolve/main/rocket-3b-2.76bpw.gguf&quot;
model = download_model(url) 
# Output: &quot;models/rocket-3b-2.76bpw.gguf&quot;</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/marcom/LlamaCpp.jl/blob/05588daebf41a644d814df4448f0a4fff69a5f4e/src/utils.jl#L1-L18">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="LlamaCpp.embeddings-Tuple{LlamaCpp.LlamaContext}" href="#LlamaCpp.embeddings-Tuple{LlamaCpp.LlamaContext}"><code>LlamaCpp.embeddings</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">embeddings(ctx) -&gt; Vector{Float32}</code></pre><p>Return the embedding, a vector of length <code>ctx.n_embd</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/marcom/LlamaCpp.jl/blob/05588daebf41a644d814df4448f0a4fff69a5f4e/src/api.jl#L46-L50">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="LlamaCpp.llama_eval-Tuple{LlamaCpp.LlamaContext, Vector{Int32}}" href="#LlamaCpp.llama_eval-Tuple{LlamaCpp.LlamaContext, Vector{Int32}}"><code>LlamaCpp.llama_eval</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">llama_eval(ctx::LlamaContext, tokens; n_past, n_threads=1)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/marcom/LlamaCpp.jl/blob/05588daebf41a644d814df4448f0a4fff69a5f4e/src/api.jl#L59-L61">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="LlamaCpp.logits-Tuple{LlamaCpp.LlamaContext}" href="#LlamaCpp.logits-Tuple{LlamaCpp.LlamaContext}"><code>LlamaCpp.logits</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">logits(ctx::LlamaContext) -&gt; Vector{Float32}</code></pre><p>Return the logits (unnormalised probabilities) for each token, a vector of length <code>ctx.n_vocab</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/marcom/LlamaCpp.jl/blob/05588daebf41a644d814df4448f0a4fff69a5f4e/src/api.jl#L69-L74">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="LlamaCpp.run_chat-Tuple{}" href="#LlamaCpp.run_chat-Tuple{}"><code>LlamaCpp.run_chat</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">run_chat(; model::AbstractString, prompt::AbstractString=&quot;&quot;, nthreads::Int=Threads.nthreads(), n_gpu_layers::Int=99, ctx_size::Int=2048, args=``)</code></pre><p>Opens an interactive console for the <code>model</code> and runs in &quot;instruction&quot; mode (especially useful for Alpaca-based models).  <code>prompt</code>, as the first message, is often used to provide instruction about the upcoming interactions (eg, style, tone, roles).</p><p>Wait for model to reply and then type your response. Press <code>Enter</code> to send the message to the model.</p><p>Interrupt the chat with <code>Ctrl+C</code></p><p>See the <a href="https://github.com/ggerganov/llama.cpp/blob/master/examples/main/README.md">full documentation</a> for more details.</p><p><strong>Arguments</strong></p><ul><li><code>model</code>: path to the model to be used</li><li><code>prompt</code>: prompt to be used. Most models expected these to be formatted in a specific way. Defaults to an empty string</li><li><code>nthreads</code>: number of threads to use. Defaults to the number of available threads</li><li><code>n_gpu_layers</code>: number of layers to offload on the GPU (a.k.a. <code>ngl</code> in llama.cpp). Requires more VRAM on your GPU but can speed up inference. Set to 0 to run inference on CPU-only. Defaults to 99 (=practically all layers)</li><li><code>ctx_size</code>: context size, ie, how big can the prompt/inference be. Defaults to 2048 (but most models allow 4,000 and more)</li></ul><p>Note: If you get odd responses AND you&#39;re using an instruction-tuned (&quot;fine-tuned&quot;), it might be that the format of your prompt is not correct.  See HuggingFace&#39;s model documentation for the correct prompt format or use a library that will do this for you (eg, PromptingTools.jl)</p><p>See also: <code>run_llama</code>, <code>run_server</code></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/marcom/LlamaCpp.jl/blob/05588daebf41a644d814df4448f0a4fff69a5f4e/src/run-programs.jl#L42-L66">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="LlamaCpp.run_llama-Tuple{}" href="#LlamaCpp.run_llama-Tuple{}"><code>LlamaCpp.run_llama</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">run_llama(; model::AbstractString, prompt::AbstractString=&quot;&quot;, nthreads::Int=1, n_gpu_layers::Int=99, ctx_size::Int=2048, args=``)</code></pre><p>Runs <code>prompt</code> through the <code>model</code> provided and returns the result. This is a single-turn version of <code>run_chat</code>.</p><p>See the <a href="https://github.com/ggerganov/llama.cpp/blob/master/examples/main/README.md">full documentation</a> for more details.</p><p><strong>Arguments</strong></p><ul><li><code>model</code>: path to the model to be used</li><li><code>prompt</code>: prompt to be used. Most models expected these to be formatted in a specific way. Defaults to an empty string</li><li><code>nthreads</code>: number of threads to use. Defaults to the number of available threads</li><li><code>n_gpu_layers</code>: number of layers to offload on the GPU (a.k.a. <code>ngl</code> in llama.cpp). Requires more VRAM on your GPU but can speed up inference. Set to 0 to run inference on CPU-only. Defaults to 99 (=practically all layers)</li><li><code>ctx_size</code>: context size, ie, how big can the prompt/inference be. Defaults to 2048 (but most models allow 4,000 and more)</li></ul><p>Note: If you get odd responses AND you&#39;re using an instruction-tuned (&quot;fine-tuned&quot;), it might be that the format of your prompt is not correct.  See HuggingFace&#39;s model documentation for the correct prompt format or use a library that will do this for you (eg, PromptingTools.jl)</p><p>See also: <code>run_chat</code>, <code>run_server</code></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/marcom/LlamaCpp.jl/blob/05588daebf41a644d814df4448f0a4fff69a5f4e/src/run-programs.jl#L3-L22">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="LlamaCpp.run_server-Tuple{}" href="#LlamaCpp.run_server-Tuple{}"><code>LlamaCpp.run_server</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">run_server(; model::AbstractString, host::AbstractString=&quot;127.0.0.1&quot;, port::Int=10897, nthreads::Int=Threads.nthreads(), 
n_gpu_layers::Int=99, ctx_size::Int=2048, args=``)</code></pre><p>Starts a simple HTTP server with the <code>model</code> provided.</p><p>Open <code>http://{host}:{port}</code> in your browser to interact with the model or use an HTTP client to send requests to the server.</p><p>Interrupt the server with <code>Ctrl+C</code>.</p><p><strong>Arguments</strong></p><ul><li><code>model</code>: path to the model to be used</li><li><code>host</code>: host address to bind to. Defaults to &quot;127.0.0.1&quot;</li><li><code>port</code>: port to listen on. Defaults to 10897</li><li><code>nthreads</code>: number of threads to use. Defaults to the number of available threads</li><li><code>n_gpu_layers</code>: number of layers to offload on the GPU (a.k.a. <code>ngl</code> in llama.cpp). Requires more VRAM on your GPU but can speed up inference. Set to 0 to run inference on CPU-only. Defaults to 99 (=practically all layers)</li><li><code>ctx_size</code>: context size, ie, how big can the prompt/inference be. Defaults to 2048 (but most models allow 4,000 and more)</li><li><code>embeddings</code>: whether to allow generating of embeddings. Defaults to <code>false</code>.  Note: Embeddings are not supported by all models and it might break the server!</li><li><code>args</code>: additional arguments to pass to the server</li></ul><p>See the <a href="https://github.com/ggerganov/llama.cpp/blob/master/examples/server/README.md">full documentation</a> for more details.</p><p><strong>Example</strong></p><p>```julia using LlamaCpp</p><p><strong>Download a model from HuggingFace, eg, Phi-2.</strong></p><p><strong>See details <a href="https://huggingface.co/TheBloke/dolphin-2_6-phi-2-GGUF">here</a></strong></p><p>using Downloads model = joinpath(&quot;models&quot;, &quot;dolphin-2<em>6-phi-2.Q6</em>K.gguf&quot;) mkpath(dirname(model)) # ensure the folder exists Downloads.download(&quot;https://huggingface.co/TheBloke/dolphin-2<em>6-phi-2-GGUF/resolve/main/dolphin-2</em>6-phi-2.Q6_K.gguf&quot;, model)</p><p><strong>go make a cup of tea while you wait... this is a 2.3GB download</strong></p><p><strong>Start the server</strong></p><p>run_server(; model)</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/marcom/LlamaCpp.jl/blob/05588daebf41a644d814df4448f0a4fff69a5f4e/src/run-programs.jl#L87-L126">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="LlamaCpp.token_to_str-Tuple{LlamaCpp.LlamaContext, Integer}" href="#LlamaCpp.token_to_str-Tuple{LlamaCpp.LlamaContext, Integer}"><code>LlamaCpp.token_to_str</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">token_to_str(ctx, token_id) -&gt; String</code></pre><p>String representation for token <code>token_id</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/marcom/LlamaCpp.jl/blob/05588daebf41a644d814df4448f0a4fff69a5f4e/src/api.jl#L101-L105">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="LlamaCpp.tokenize-Tuple{LlamaCpp.LlamaContext, AbstractString}" href="#LlamaCpp.tokenize-Tuple{LlamaCpp.LlamaContext, AbstractString}"><code>LlamaCpp.tokenize</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">tokenize(ctx :: LlamaContext, text :: AbstractString) -&gt; Vector{llama_token}</code></pre><p>Tokenizes <code>text</code> according to the <code>LlamaContext</code> <code>ctx</code> and returns a <code>Vector{llama_token}</code>, with <code>llama_token == Int32</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/marcom/LlamaCpp.jl/blob/05588daebf41a644d814df4448f0a4fff69a5f4e/src/api.jl#L83-L88">source</a></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../">« Home</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.8.1 on <span class="colophon-date" title="Saturday 22 February 2025 16:28">Saturday 22 February 2025</span>. Using Julia version 1.11.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
